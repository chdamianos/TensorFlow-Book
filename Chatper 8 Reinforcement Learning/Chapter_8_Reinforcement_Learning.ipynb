{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch `08`: Concept `01` Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical explanation\n",
    "\n",
    "### Introduction\n",
    "RL is about training a NN to be able to decide on an action based on the best chances to get a reward. This reward doesn't need to be immediate, i.e. it can anticipate long term changes and adapt to gain long-term rewards rather than short-term. \n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### Step 1 - Initialize\n",
    "Initialise algorithm so that there is an initial state. In our example the state is a vector with the prices of stocks over a time period (in the \"past\"), the available budget and the number of shares we have bought. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{state} =   \\begin{pmatrix}\n",
    "  price_{1} \\\\\n",
    "  price_{2} \\\\\n",
    "  \\vdots \\\\\n",
    "  price_{n} \\\\\n",
    "  budget  \\\\\n",
    "  share_{number}\n",
    " \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The size of the time period in the past that we are going to consider is a hyperparameter that determines the size of $\\mathbf{state}$.\n",
    "\n",
    "In the code the state is initialised in function `run_simulation`\n",
    "\n",
    "```python\n",
    "current_state = np.asmatrix(np.hstack((prices[i:i+hist], budget, num_stocks)))\n",
    "\n",
    "```\n",
    "#### Step 2 - Select action\n",
    "Selection action to perform. The action is selected either randomly or using the NN model. In the start of algorithm steps we want to do more `exploration` rather than `exploitation` since at first our model will not know that much.\n",
    "\n",
    "This step is performed in the `select_action` method\n",
    "\n",
    "```python\n",
    "def select_action(self, current_state, step):\n",
    "    threshold = min(self.epsilon, step / 1000.)\n",
    "    if random.random() < threshold:\n",
    "        action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})\n",
    "        action_idx = np.argmax(action_q_vals)\n",
    "        action = self.actions[action_idx]\n",
    "    else:\n",
    "        action = self.actions[random.randint(0, len(self.actions) - 1)]\n",
    "    return action\n",
    "```\n",
    "\n",
    "The important code line is `action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})` here we run our state through the NN model and get back the `relu` activation values. These are trained to predict the `utility` of each action, which is our `reward` taking into account long-term rewards. `action_q_vals` is a vector with size equal to the size of our actions. So each element represents the potential `utility` of taking an action. We take the action that can maximize the `utility` using `action_idx = np.argmax(action_q_vals)`.\n",
    "\n",
    "#### Step 3 - Calculate new state\n",
    "\n",
    "Based on the results of step 2, we take an action. In the case of our example it's either `sell`, `buy`, `hold`. So if the action is to `buy` we buy the stock at it's current share value, which is not part of the $\\mathbf{state}$. \n",
    "\n",
    "$current_{value}=price_{n+1}$.\n",
    "\n",
    "We calculate the new state based on our action.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{state_{new}} =   \\begin{pmatrix}\n",
    "  price_{2} \\\\\n",
    "  price_{3} \\\\\n",
    "  \\vdots \\\\\n",
    "  price_{n+1} \\\\\n",
    "  budget_{new}  \\\\\n",
    "  share_{number_{new}}\n",
    " \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### Step 4 - Calculate reward\n",
    "\n",
    "Now we calculate our `reward`, this is our immediate gain.\n",
    "\n",
    "$reward=(budget_{new}+share_{number_{new}} \\times current_{value}) - (budget_{old}+share_{number_{old}} \\times current_{value})$\n",
    "\n",
    "#### Step 5 - Correct utility of current action\n",
    "\n",
    "Based on $\\mathbf{state_{old}}$ and $\\mathbf{state_{new}}$ we update our `policy`. This is the part we train the NN model. \n",
    "\n",
    "This step takes place in the `update_q` function\n",
    "```python\n",
    "    def update_q(self, state, action, reward, next_state, writer, i):\n",
    "        action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})\n",
    "        next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})\n",
    "        next_best_action_idx = np.argmax(next_action_q_vals)\n",
    "        current_action_idx = self.actions.index(action)\n",
    "        action_q_vals[0, current_action_idx] = reward + \\\n",
    "                                self.gamma * next_action_q_vals[0, next_best_action_idx]\n",
    "        action_q_vals = np.squeeze(np.asarray(action_q_vals))\n",
    "        _, lossSumm = self.sess.run([self.train_op, self.loss_summary], \n",
    "                                    feed_dict={self.x: state, self.y: action_q_vals})\n",
    "```\n",
    "\n",
    "We get our current `utility` function values for each action using `action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})`(this is done in the `select_action` function also). \n",
    "\n",
    "Then using our calculated $\\mathbf{state_{new}}$ we get the `utility` function values for each action based on our new state, which depends on the previous state: `next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})`\n",
    "\n",
    "We calculate, based on the utility function values, which is the best **next action** to take: `next_best_action_idx = np.argmax(next_action_q_vals)`. This allows to \"correct\" the reward of our **current action** based on long term consequences.\n",
    "\n",
    "The correction is performed using the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "utility_{current} =  reward + \\gamma \\times utility_{next}\n",
    "\\end{equation*}\n",
    "\n",
    "**Note that this changes the utility of the current action only, so only an element of the `action_q_vals`**\n",
    "\n",
    "If $\\gamma$ is zero then we don't take into account long term consequences of our actions. \n",
    "This equation is implemented by `action_q_vals[0, current_action_idx] = reward + self.gamma * next_action_q_vals[0, next_best_action_idx]`.\n",
    "\n",
    "We could also generalize and take into account more actions than just the next, i.e. the one after next etc.\n",
    "\n",
    "\\begin{equation*}\n",
    "utility_{0} =  reward + \\gamma_{1} \\times utility_{1} + \\gamma_{2} \\times utility_{2} +... + \\gamma_{k} \\times utility_{k}\n",
    "\\end{equation*}\n",
    "\n",
    "There is also another equation that intoduces another parameter $\\alpha$, which aims to make newly available information less/more important than historical records.\n",
    "\n",
    "\\begin{equation*}\n",
    "utility_{current} =  (1-\\alpha) \\times utility_{current}+\\alpha \\times (reward + \\gamma \\times utility_{next})\n",
    "\\end{equation*}\n",
    "\n",
    "If $\\alpha$ is increased then we expect our `agent` (the decision maker) to learn to solve tasks faster but not optimal.If $\\alpha$ is decreased our `agent` is allowed more time to explore and exploit. \n",
    "\n",
    "#### Step 6 - Train NN model\n",
    "\n",
    "Now we train our NN based on the input, our current state `state` and the corrected `utility` values of our current action that takes into account future consequences.\n",
    "\n",
    "The training call is in function `_, lossSumm = self.sess.run([self.train_op, self.loss_summary],feed_dict={self.x: state, self.y: action_q_vals})`. This will update our NN model parameters so that our state produces `relu` activations that better agree with the corrected utility values of our current action `action_q_vals`, rather than the original that were derived from the same input `state`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **states** are previous history of stock prices, current budget, and current number of shares of a stock.\n",
    "\n",
    "The **actions** are buy, sell, or hold (i.e. do nothing).\n",
    "\n",
    "The stock market data comes from the Yahoo Finance library, `pip install yahoo-finance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from yahoo_finance import Share\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an abstract class called `DecisionPolicy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a good idea to create a class with the relevant methods to\n",
    "# reference later, such as an abstract class or interface\n",
    "class DecisionPolicy:\n",
    "    # Basically, reinforcement learning needs two operations well defined:\n",
    "    def select_action(self, current_state, step):\n",
    "        # (1) how to select an action\n",
    "        pass\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state):\n",
    "        # (2) how to improve the utility Q-function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one way we could implement the decision policy, called a random decision policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inherit from `DecisionPolicy` superclass to implement a random decision policy\n",
    "# it's a good idea to test our poly against a random policy or a \"greedy\" policy (not implemented) that \n",
    "# just takes the action that maximizes current reward\n",
    "class RandomDecisionPolicy(DecisionPolicy):# Inherit from DecisionPolicy to implement its functions\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, current_state, step):\n",
    "        # Randomly choose the next action\n",
    "        action = random.choice(self.actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good baseline. Now let's use a smarter approach using a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningDecisionPolicy(DecisionPolicy):\n",
    "    def __init__(self, actions, input_dim):\n",
    "        # Set the hyper-parameters from the Q-function\n",
    "        self.epsilon = 0.95\n",
    "        self.gamma = 0.3\n",
    "        self.actions = actions\n",
    "        output_dim = len(actions)\n",
    "        # Set the number of hidden nodes in the neural networks\n",
    "        h1_dim = 20\n",
    "        # Define the input and output tensors\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_dim])\n",
    "        self.y = tf.placeholder(tf.float32, [output_dim])\n",
    "        # Design the neural network architecture\n",
    "        W1 = tf.Variable(tf.random_normal([input_dim, h1_dim]))\n",
    "        b1 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))\n",
    "        h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)\n",
    "        W2 = tf.Variable(tf.random_normal([h1_dim, output_dim]))\n",
    "        b2 = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n",
    "        # Define the op to compute the utility\n",
    "        self.q = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "        # Set the loss as the square error\n",
    "        self.loss = tf.square(self.y - self.q)\n",
    "        # set loss summary for tensorboard\n",
    "        self.loss_summary = tf.summary.scalar('loss', tf.reduce_mean(self.loss))\n",
    "        # Use an optimizer to update model parameters to minimize the loss\n",
    "        self.train_op = tf.train.AdagradOptimizer(0.01).minimize(self.loss)\n",
    "        # Set up the session and initialize variables\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def select_action(self, current_state, step):\n",
    "        # select minimum between epsilon and step/1000 because\n",
    "        # at the begining we want to explore so that our algorithm learns\n",
    "        # before it can make policies\n",
    "        threshold = min(self.epsilon, step / 1000.)\n",
    "        if random.random() < threshold:\n",
    "            # Exploit best option with probability epsilon\n",
    "            action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})\n",
    "            action_idx = np.argmax(action_q_vals)  # TODO: replace w/ tensorflow's argmax\n",
    "            action = self.actions[action_idx]\n",
    "        else:\n",
    "            # Explore random option with probability 1 - epsilon\n",
    "            action = self.actions[random.randint(0, len(self.actions) - 1)]\n",
    "        return action\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state, writer, i):\n",
    "        \"\"\"Update the Q-function by updating its model parameters\"\"\"\n",
    "        # calculate utilities of each action for current state\n",
    "        action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})\n",
    "        # calculate utilities of each action for next state\n",
    "        next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})\n",
    "        # find next action that will maximise the utility when we are in the next state\n",
    "        next_best_action_idx = np.argmax(next_action_q_vals)\n",
    "        # find index of current action\n",
    "        current_action_idx = self.actions.index(action)\n",
    "        # now we need to \"correct\" the utility of current action by looking at the utility of next best action\n",
    "        # conditional on the current state. In other words the next state depends on the current state.\n",
    "        # the higher the gamma hyperparameter the more we take into account future rewards for current actions\n",
    "        # if gamma is zero then we ignore any long term rewards (consequences of our actions)\n",
    "        # I guess we could add more future states and best actions to make our algorithm better at long term actions\n",
    "        # i.e. action_q_vals[0, current_action_idx] = reward + self.gamma1 * next_action_q_vals[0, next_best_action_idx]\n",
    "        # + self.gamma2 * next_next_action_q_vals[0, next_next_best_action_idx]\n",
    "        action_q_vals[0, current_action_idx] = reward + \\\n",
    "                                               self.gamma * next_action_q_vals[0, next_best_action_idx]\n",
    "        action_q_vals = np.squeeze(np.asarray(action_q_vals))\n",
    "        # have the nn learn a function that minimizes the difference between the \n",
    "        # current uncorrected action and the corrected\n",
    "        # ones based on future consequences\n",
    "        _, lossSumm = self.sess.run([self.train_op, self.loss_summary], \n",
    "                                    feed_dict={self.x: state, self.y: action_q_vals})\n",
    "        writer.add_summary(summary=lossSumm, global_step=i)\n",
    "        writer.add_graph(self.sess.graph)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run a simulation of buying and selling stocks from a market:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(policy, initial_budget, initial_num_stocks, prices, hist, writer):\n",
    "    # Initialize values that depend on computing the net worth of a portfolio\n",
    "    budget = initial_budget\n",
    "    num_stocks = initial_num_stocks\n",
    "    share_value = 0\n",
    "    transitions = list()\n",
    "    for i in tqdm.tqdm(range(len(prices) - hist - 1)):\n",
    "        # The state is a `hist+2` dimensional vector. We’ll force it to by a numpy matrix.\n",
    "        current_state = np.asmatrix(np.hstack((prices[i:i+hist], budget, num_stocks)))\n",
    "        # Calculate the portfolio value\n",
    "        current_portfolio = budget + num_stocks * share_value\n",
    "        # Select an action from the current policy\n",
    "        action = policy.select_action(current_state, i)\n",
    "        share_value = float(prices[i + hist])\n",
    "        # Update portfolio values based on action\n",
    "        if action == 'Buy' and budget >= share_value:\n",
    "            budget -= share_value\n",
    "            num_stocks += 1\n",
    "        elif action == 'Sell' and num_stocks > 0:\n",
    "            budget += share_value\n",
    "            num_stocks -= 1\n",
    "        else:\n",
    "            action = 'Hold'\n",
    "        # Compute new portfolio value after taking action\n",
    "        new_portfolio = budget + num_stocks * share_value\n",
    "        # Compute the reward from taking an action at a state\n",
    "        reward = new_portfolio - current_portfolio\n",
    "        next_state = np.asmatrix(np.hstack((prices[i+1:i+hist+1], budget, num_stocks)))\n",
    "        transitions.append((current_state, action, reward, next_state))\n",
    "        # Update the policy after experiencing a new action\n",
    "        # This is when we train our model\n",
    "        policy.update_q(current_state, action, reward, next_state, writer, i)\n",
    "    # Compute final portfolio worth\n",
    "    portfolio = budget + num_stocks * share_value\n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run simulations multiple times and average out the performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations(policy, budget, num_stocks, prices, hist):\n",
    "    # Decide number of times to re-run the simulations\n",
    "    num_tries = 3\n",
    "    # Store portfolio worth of each run in this array\n",
    "    final_portfolios = list()\n",
    "    # initial portfolio value\n",
    "    portfolio = budget\n",
    "    for i in range(num_tries):\n",
    "        # tensorboard writer\n",
    "        # *** TENSORBOARD ***\n",
    "        # set directory to collect saved summary tensors with each run\n",
    "        # based on run time\n",
    "        now = dt.datetime.now()\n",
    "        currentDir = \"./logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "        # create writer and set directory and graph\n",
    "        writer = tf.summary.FileWriter(currentDir)\n",
    "        print('Running simulation {}...'.format(i + 1))\n",
    "        print('Starting budget portfolio: ${}'.format(portfolio))\n",
    "        final_portfolio = run_simulation(policy, budget, num_stocks, prices, hist, writer)\n",
    "        final_portfolios.append(final_portfolio)\n",
    "        print('Final portfolio: ${}'.format(final_portfolio))\n",
    "        writer.close()\n",
    "    # Average the values from all the runs\n",
    "    avg, std = np.mean(final_portfolios), np.std(final_portfolios)\n",
    "    return avg, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the following function to use the Yahoo Finance library and obtain useful stockmarket data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prices(share_symbol, start_date, end_date, cache_filename='stock_prices.npy'):\n",
    "    try:\n",
    "        stock_prices = np.load(cache_filename)\n",
    "    except IOError:\n",
    "        share = Share(share_symbol)\n",
    "        stock_hist = share.get_historical(start_date, end_date)\n",
    "        stock_prices = [stock_price['Open'] for stock_price in stock_hist]\n",
    "        np.save(cache_filename, stock_prices)\n",
    "\n",
    "    return stock_prices.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who wants to deal with stock market data without looking a pretty plots? No one. So we need this out of law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prices(prices):\n",
    "    plt.title('Opening stock prices')\n",
    "    plt.xlabel('day')\n",
    "    plt.ylabel('price ($)')\n",
    "    plt.plot(prices)\n",
    "    plt.savefig('prices.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a reinforcement learning policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/5846 [00:00<00:42, 135.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation 1...\n",
      "Starting budget portfolio: $1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5846/5846 [00:39<00:00, 146.70it/s]\n",
      "  0%|          | 19/5846 [00:00<00:30, 188.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final portfolio: $1573.7650420000004\n",
      "Running simulation 2...\n",
      "Starting budget portfolio: $1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5846/5846 [00:36<00:00, 170.98it/s]\n",
      "  0%|          | 21/5846 [00:00<00:27, 209.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final portfolio: $1617.4550170000002\n",
      "Running simulation 3...\n",
      "Starting budget portfolio: $1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5846/5846 [00:36<00:00, 160.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final portfolio: $1620.3799610000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    prices = get_prices('MSFT', '1992-07-22', '2016-07-22')\n",
    "    plot_prices(prices)\n",
    "    # Define the list of actions the agent can take\n",
    "    actions = ['Buy', 'Sell', 'Hold']\n",
    "    hist = 200\n",
    "    # Initial a random decision policy\n",
    "    # policy = RandomDecisionPolicy(actions)\n",
    "    policy = QLearningDecisionPolicy(actions, hist + 2)\n",
    "    # Set the initial amount of money available to use\n",
    "    budget = 1000.0\n",
    "    # Set the number of stocks already owned\n",
    "    num_stocks = 0\n",
    "    # Run simulations multiple times to compute expected value of final net worth\n",
    "    run_simulations(policy, budget, num_stocks, prices, hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfCPU]",
   "language": "python",
   "name": "conda-env-tfCPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
