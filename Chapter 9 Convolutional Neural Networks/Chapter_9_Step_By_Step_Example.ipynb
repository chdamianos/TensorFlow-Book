{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of convolution nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Input reshape\n",
    "First we reshape our data so they can pass through the `tf.conv2d` function.\n",
    "According the documentation of `tf.conv2d` the input needs to be as `[batch, in_height, in_width, in_channels]`. Since in this case we are doing a simple pass through example with one image we will reshape our input: \n",
    "```python \n",
    "idx=4 # selected example from data\n",
    "exampleData = data[idx, :]\n",
    "# batch=1, in_height=24, in_width=24, in_channels=1 (since it's greyscale)\n",
    "exampleDataReshaped = tf.reshape(exampleData, shape=[-1, 24, 24, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Convolution\n",
    "The reshaped image is then passed through the convolution filter:\n",
    "```python\n",
    "conv = tf.nn.conv2d(exampleDataReshaped, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv_with_b = tf.nn.bias_add(conv, b)\n",
    "conv_out = tf.nn.relu(conv_with_b)\n",
    "```\n",
    "\n",
    "`W` `b` are `tf` variables that hold the weights to be trained. These define the output shape of `conv_out`. For example if:\n",
    "```python\n",
    "W = tf.Variable(tf.random_normal([5, 5, 1, 64]))\n",
    "b = tf.Variable(tf.random_normal([64]))\n",
    "```\n",
    "where `[5, 5, 1, 64]`: 5,5 -> size of filter on image, 1-> input dimension (1 for grayscale, 3 for RGB), 64 -> number of convolutions\n",
    "\n",
    "The result of this step is an array with shape `(1, 24, 24, 64)`. This holds 64 24x24 filtered images that depend on `W` and `b` (which are trainable). These images can be considered image processing features extracted from the original image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Max Pool\n",
    "We can prevent overfit by artifically reducing the numer of parameters of our model. This can be done by taking the max value of 2,3 or more adjacent array elements. This is done using `tf.nn.max_pool`. For example the result of the previous example can be sampled by `tf.nn.max_pool` to half its size:\n",
    "\n",
    "```python\n",
    "maxPool = tf.nn.max_pool(conv_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "```\n",
    "The result is that the filtered images have now a shape of `(1, 12, 12, 64)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Normalization\n",
    "The result of step 3 can be normalized using `tf.nn.lrn` which takes the input tensor (shape `(1, 12, 12, 64)`) and treats it as a 3D array of 1D vectors (along the last dimension), and each vector is normalized independently. Within a given vector, each component is divided by the weighted, squared sum of inputs within `depth_radius`. \n",
    "\n",
    "So if we take one of these vectors they would look like `vec1=maxPool[:,:,:,0], vec2=maxPool[:,:,:,1],...` and these are the filtered images. For each filtered image we perform normalisation as described at  https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization.\n",
    "\n",
    "In the example this is performed as:\n",
    "```python\n",
    "norm = tf.nn.lrn(maxPool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "```\n",
    "**These sort of normalisation techniques seem to have fallen out of favour such as dropout and batch normalization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Fully connected layer\n",
    "Steps 2-4 can be repeated. At the end we will pass the results through a fully connected layer. In the example this operation by:\n",
    "\n",
    "```python\n",
    "norm_reshaped = tf.reshape(norm, [-1, W2.get_shape().as_list()[0]])\n",
    "full = tf.add(tf.matmul(norm_reshaped, W2), b2)\n",
    "full_out = tf.nn.relu(full) \n",
    "```\n",
    "`W2` has a shape that matches the flatten dimensions of the ouput of the convolution steps. For example if the output shape of norm is `(1, 12, 12, 64)`. Then `W2` will have a shape of `(1*12*12*64, 1024)` where `1024` is just the output dimensions of the fully connected layer.\n",
    "\n",
    "**NOTE: The variable names are general and DO NOT refer to the example code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Final output\n",
    "Finally we need to produce a result in the same shape as our target (labels), which are one hot encoded. In the example we have we have only one label, since we are passing a single image through our nn model.   \n",
    "\n",
    "```python\n",
    "out = tf.add(tf.matmul(full_out, W_out), b_out) \n",
    "```\n",
    "`Wout` has a shape that matches the target. In our example the target is a one-hot encoded vector of 10 different labels so `Wout` has shape `(1024,10)`\n",
    "\n",
    "**NOTE: The variable names are general and DO NOT refer to the example code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Cost\n",
    "To be able to train our nn model we need a cost between the targets (labels) and the output of the model. For this we use the `tf.nn.softmax_cross_entropy_with_logits` function:\n",
    "\n",
    "```python\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=target))\n",
    "```\n",
    "\n",
    "**NOTE: The variable names are general and DO NOT refer to the example code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "(50000, 3072) (50000,)\n",
      "shape of input is (1, 24, 24, 1)\n",
      "shape of first convolution is (1, 24, 24, 64)\n",
      "shape of first max pool is (1, 12, 12, 64)\n",
      "shape of first normalisation is (1, 12, 12, 64)\n",
      "shape of second convolution is (1, 12, 12, 64)\n",
      "shape of second normalisation is (1, 12, 12, 64)\n",
      "shape of second max pool is (1, 6, 6, 64)\n",
      "shape of third reshaped max pool is (1, 2304)\n",
      "shape of third nn weight array is (2304, 1024)\n",
      "shape of third fully connected layer is (1, 1024)\n",
      "shape of final output is (1, 10)\n",
      "WARNING:tensorflow:From <ipython-input-1-c55c61553504>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "model output is [[1. 0. 1. 0. 1. 0. 1. 1. 1. 0.]]\n",
      "target is [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cost is 3.0110981464385986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cifar_tools_DC\n",
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "names, data, labels = \\\n",
    "    cifar_tools_DC.read_data(\n",
    "        '/media/damianos/New Volume/Desktop/Projects/Machine Learning with Tensorflow/TensorFlow-Book/ch09_cnn/cifar-10-batches-py')\n",
    "\n",
    "# have variables outside of session \n",
    "W1 = tf.Variable(tf.random_normal([5, 5, 1, 64]))\n",
    "b1 = tf.Variable(tf.random_normal([64]))\n",
    "W2 = tf.Variable(tf.random_normal([5, 5, 64, 64]))\n",
    "b2 = tf.Variable(tf.random_normal([64]))\n",
    "W3 = tf.Variable(tf.random_normal([6*6*64, 1024]))\n",
    "b3 = tf.Variable(tf.random_normal([1024]))\n",
    "W_out = tf.Variable(tf.random_normal([1024, len(names)]))\n",
    "b_out = tf.Variable(tf.random_normal([len(names)]))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    onehot_labels = tf.one_hot(labels, len(names), on_value=1., off_value=0., axis=-1)\n",
    "    onehot_labels_eval = sess.run(onehot_labels)\n",
    "    # select an example image and an example label to process\n",
    "    exampleData = data[4, :]\n",
    "    exampleLabel = onehot_labels_eval[4, :]\n",
    "    # reshape input to be able to pass through tf.conv2d    \n",
    "    exampleDataReshaped = tf.reshape(exampleData, shape=[-1, 24, 24, 1])\n",
    "    print('shape of input is {}'.format(sess.run(exampleDataReshaped).shape))\n",
    "    # use a 5x5 filter to produce 64 different filtered images\n",
    "    conv1 = tf.nn.conv2d(exampleDataReshaped, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv_with_b1 = tf.nn.bias_add(conv1, b1)\n",
    "    conv_out1 = tf.nn.relu(conv_with_b1)\n",
    "    print('shape of first convolution is {}'.format(sess.run(conv_out1).shape))\n",
    "    ## max pool\n",
    "    k = 2\n",
    "    max1 = tf.nn.max_pool(conv_out1, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "    print('shape of first max pool is {}'.format(sess.run(max1).shape))\n",
    "    ## normalisation\n",
    "    norm1 = tf.nn.lrn(max1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    print('shape of first normalisation is {}'.format(sess.run(norm1).shape))\n",
    "    # repeat another convolution\n",
    "    conv2 = tf.nn.conv2d(norm1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv_with_b2 = tf.nn.bias_add(conv2, b2)\n",
    "    conv_out2 = tf.nn.relu(conv_with_b2)\n",
    "    print('shape of second convolution is {}'.format(sess.run(conv_out2).shape))\n",
    "    ## normalisation\n",
    "    norm2 = tf.nn.lrn(conv_out2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    print('shape of second normalisation is {}'.format(sess.run(norm2).shape)) \n",
    "    ## max pool\n",
    "    max2 = tf.nn.max_pool(norm2, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "    print('shape of second max pool is {}'.format(sess.run(max2).shape))\n",
    "    ## reshape last result (max2) to be passed through a fully connected layer\n",
    "    ## whose shape equals the flatten shape of max2 and the shape of the output \n",
    "    ## from the fully connected layer\n",
    "    max3_reshaped = tf.reshape(max2, [-1, W3.get_shape().as_list()[0]])\n",
    "    print('shape of third reshaped max pool is {}'.format(sess.run(max3_reshaped).shape))\n",
    "    print('shape of third nn weight array is {}'.format(sess.run(W3).shape))\n",
    "    local3 = tf.add(tf.matmul(max3_reshaped, W3), b3)\n",
    "    local_out3 = tf.nn.relu(local3) \n",
    "    print('shape of third fully connected layer is {}'.format(sess.run(local_out3).shape))\n",
    "    # final output\n",
    "    out = tf.nn.sigmoid(tf.add(tf.matmul(local_out3, W_out), b_out))\n",
    "    print('shape of final output is {}'.format(sess.run(out).shape))\n",
    "    # cost function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=exampleLabel))\n",
    "    print('model output is {}'.format(sess.run(out)))\n",
    "    print('target is {}'.format(exampleLabel))\n",
    "    print('cost is {}'.format(sess.run(cost)))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
